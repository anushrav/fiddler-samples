{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddler Quick Start Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide will walk you through the basic onboarding steps required to use Fiddler for production model monitoring and explainability. API documentation can be found [here](https://docs.fiddler.ai/api-reference/python-package/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Zero: Packages and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid import misses, we will have most package imports in this section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiddler as fdl\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import shutil\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step One: Client Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize the client object by specifying:\n",
    "- The `url`: url is the fiddler URL that you have been provided to access. Usually of the form ‘XXXXX.fiddler.ai’. Contact us if you don’t have it\n",
    "- The `org_id`: organization id is an identifier for the account. See Fiddler_URL/settings/general to find this id (listed as \"Organization ID\")\n",
    "<img src=\"images/org_id.png\" width=800 height=800 />\n",
    "- The `auth_token`: this token is used to authenticate access. See Fiddler_URL/settings/credentials to find, create, or change this token\n",
    "<img src=\"images/auth_token.png\" width=800 height=800 />\n",
    "\n",
    "You can also save this config as a file called `fiddler.ini` in the same folder as the notebook/script. That saves you from specifying the parameters in every notebook and script.\n",
    "<img src=\"images/fiddler_ini.png\" width=800 height=800 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiddler-client==0.6.18;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fiddler.ini\n",
    "\n",
    "[FIDDLER]\n",
    "url = https://your-org@fiddler.ai\n",
    "org_id = [YOUR ORG URL]\n",
    "auth_token = [YOUR ORG TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiddler as fdl\n",
    "\n",
    "# client = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=auth_token)\n",
    "client = fdl.FiddlerApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Two: Create Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a project, a convenient container for housing the models and datasets associated with a given ML use case.\n",
    "\n",
    "For the purposes of a full quick start, it is best to create a `project_id` with a unique name to best track your progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'quickstart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our project using project_id\n",
    "if project_id not in client.list_projects():\n",
    "    client.create_project(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Three: Upload Baseline Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will upload the datasets that will serve as baselines for various product capabilities, including monitoring of model performance, prediction & feature drift, and data errors; generating prediction-level (point) and model-level (global) explanations; and calculating various bias metrics.\n",
    "\n",
    "We recommend using the model's training set for the most faithful and actionable metrics. In addition to the model's features and labels, Fiddler requires a few additional attributes to unlock its full suite of capabilities:\n",
    "\n",
    "*   Model predictions (Mandatory: serves as a baseline for prediction drift)\n",
    "*   Model decisions* (Optional: used to monitor model decsions over time, e.g. loan approved vs denied. The data uploaded initially can be random)\n",
    "*  Model metadata* (Any additional fields relevant for model analysis. In the event you intend to use Fiddler to detect model bias, include any relevant protected attributes here, e.g. gender, race, age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data you are going to use for training your model. For this tutorial, we will be using an auto insurance dataset that can be found [here](https://www.kaggle.com/somjee/auto-insurance-customerlifetimevalue?select=data.csv). \n",
    "\n",
    "**Note**: We are also adding a `high_value` field to act as our decision column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/somjee/auto-insurance-customerlifetimevalue?select=data.csv\n",
    "df = pd.read_csv('/app/fiddler_samples/samples/datasets/auto_insurance/data.csv')\n",
    "df = df.rename(columns={\"State\": \"Location State\"})\n",
    "df.columns = [x.lower().replace(' ', '_') for x in df.columns]\n",
    "\n",
    "# Adding a decision column to our data. In this case, we deem a 'high_value' customer as\n",
    "# one with customer_lifetime_value >= 5000\n",
    "df = df.assign(high_value=['Yes' if x >= 5000 else 'No' for x in df['customer_lifetime_value']])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into Train/Test\n",
    "\n",
    "Now we will split our dataset into a train/test set to be used in training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.8,random_state=200)\n",
    "df_test = df.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'auto_insurance'\n",
    "dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a schema for our dataset, and upload the dataset to Fiddler. \n",
    "\n",
    "If the `dataset_id` was uploaded previously, we can fetch and use the schema from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve dataset if already uploaded\n",
    "if dataset_id in client.list_datasets(project_id):\n",
    "    df_schema = client.get_dataset_info(project_id, dataset_id)\n",
    "else:\n",
    "    df_schema = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=1000)\n",
    "    upload_result = client.upload_dataset(\n",
    "        project_id=project_id,\n",
    "        dataset={'train': df_train,\n",
    "                 'test': df_test},\n",
    "        dataset_id=dataset_id,\n",
    "        info=df_schema)\n",
    "\n",
    "\n",
    "df_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Four: Register Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, in the dataset upload step we did not ask for the model’s features and targets, or any model specific information. That’s because we allow for linking multiple models to a given dataset schema. Hence we require an Infer model schema step which helps us know the features relevant to the model and the model task. Here you can specify the input features, the target column, decision columns and metadata columns, and also the type of model.\n",
    "- We can infer the model task from the target column, or it can explicitly set. Currently we support three model types:\n",
    "    - Regression\n",
    "    - Binary Classification\n",
    "    - Multi-class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'ltv_regressor'\n",
    "target = 'customer_lifetime_value'\n",
    "continuous_features = ['income', 'monthly_premium_auto', 'months_since_last_claim', 'months_since_policy_inception',\n",
    "                        'number_of_open_complaints', 'number_of_policies', 'total_claim_amount']\n",
    "categorical_features = ['location_state', 'employmentstatus', 'policy_type', 'policy', 'vehicle_class','vehicle_size']\n",
    "\n",
    "feature_columns = list(continuous_features + categorical_features)\n",
    "metadata_cols = ['gender']\n",
    "decision_cols = ['high_value']\n",
    "outputs = ['predicted_customer_lifetime_value']\n",
    "\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=client.get_dataset_info(project_id, dataset_id),\n",
    "    target=target, \n",
    "    features=feature_columns,\n",
    "    metadata_cols=metadata_cols,\n",
    "    decision_cols=decision_cols,\n",
    "    outputs=outputs,\n",
    "    input_type=fdl.ModelInputType.TABULAR,\n",
    "    model_task=fdl.ModelTask.REGRESSION,\n",
    "    display_name='Gradient Boosting Regressor',\n",
    "    description='this is a GradientBoostingRegressor model from the tutorial',\n",
    ")\n",
    "\n",
    "model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete model if it is already available\n",
    "client.delete_model(project_id, model_id, delete_prod=True)\n",
    "\n",
    "# register model\n",
    "client.register_model(project_id, model_id, dataset_id, model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Five: Simulate Monitoring Traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming data example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will be simulating traffic to send for our model monitoring by using [publish_event](https://docs.fiddler.ai/api-reference/python-package/#publish-event). This will be the equivalent of running our model separately on some data, and either sending to Fiddler then, or saving this information to a log and sending at a later point.\n",
    "\n",
    "For this demonstration, we will be going with a streaming approach. We will utilize a log containing rows with fields corresponding to:\n",
    "\n",
    "- inputs \n",
    "- predictions\n",
    "- labels (targets)\n",
    "- decisions\n",
    "\n",
    "We can find the fields that will be utilized by consulting our `ModelInfo` object:\n",
    "\n",
    "```\n",
    "ModelInfo:\n",
    "      display_name: Gradient Boosting Regressor \\\n",
    "      description: this is a GradientBoostingRegressor model from the tutorial\n",
    "      input_type: ModelInputType.TABULAR\n",
    "      model_task: ModelTask.REGRESSION\n",
    "-->   inputs:\n",
    "                                   column     dtype count(possible_values)  \\\n",
    "        0                  location_state  CATEGORY                      5   \n",
    "        1                employmentstatus  CATEGORY                      5   \n",
    "        2                          income   INTEGER                          \n",
    "        3            monthly_premium_auto   INTEGER                          \n",
    "        4         months_since_last_claim   INTEGER                          \n",
    "        5   months_since_policy_inception   INTEGER                          \n",
    "        6       number_of_open_complaints   INTEGER                          \n",
    "        7              number_of_policies   INTEGER                          \n",
    "        8                     policy_type  CATEGORY                      3   \n",
    "        9                          policy  CATEGORY                      9   \n",
    "        10             total_claim_amount     FLOAT                          \n",
    "        11                  vehicle_class  CATEGORY                      6   \n",
    "        12                   vehicle_size  CATEGORY                      3                    \n",
    "-->   outputs\n",
    "                                      column  dtype count(possible_values)  \\\n",
    "        0  predicted_customer_lifetime_value  FLOAT                          \n",
    "\n",
    "          is_nullable value_range  \n",
    "        0       False       * - *  \n",
    "      metadata:\n",
    "           column     dtype  count(possible_values) is_nullable value_range\n",
    "        0  gender  CATEGORY                       2       False            \n",
    "-->   decisions:\n",
    "               column     dtype  count(possible_values) is_nullable value_range\n",
    "        0  high_value  CATEGORY                       2       False            \n",
    "-->   targets: [Column(name=\"customer_lifetime_value\", data_type=DataType.FLOAT, possible_values=None, \n",
    "                is_nullable=False, value_range_min=1898.007675, value_range_max=83325.38119)]\n",
    "                  misc:{}\n",
    "    \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_log = pd.read_csv('/app/fiddler_samples/samples/datasets/auto_insurance/event_log.csv')\n",
    "event_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will publish these rows as events. To most accurately simulate this as a time-series event, we will also be calling a function to generate a timestamp in the last 2 weeks. Real data will ideally have a timestamp related to when the event took place; otherwise, the current time will be used.\n",
    "\n",
    "**Note**: The timestamp must be in UTC milliseconds. See [here](https://docs.fiddler.ai/api-reference/python-package/#publish-event) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from random import sample, randint\n",
    "NUM_EVENTS_TO_SEND = 500\n",
    "\n",
    "def getTimestampsFromPastTwoWeeks():\n",
    "    \"\"\"\n",
    "    Generate a list of timestamps from the past two weeks. Timestamp is in \n",
    "    milliseconds since epoch in UTC.\n",
    "    \"\"\"\n",
    "    TWO_WEEKS_MS = 604800 * 2 * 1000\n",
    "    current_time_in_ms = round(time.time() * 1000)\n",
    "    lower = current_time_in_ms - TWO_WEEKS_MS\n",
    "    upper = current_time_in_ms\n",
    "    length = NUM_EVENTS_TO_SEND\n",
    "    timestamps = [lower + x*(upper-lower)/length for x in range(length)]\n",
    "\n",
    "    return timestamps\n",
    "\n",
    "        \n",
    "# Convert this dataframe into a list of dictionary events, where each event is its own dictionary\n",
    "event_list_dict = event_log.sample(n=NUM_EVENTS_TO_SEND).to_dict(orient='records') \n",
    "event_ms_time_stamps = getTimestampsFromPastTwoWeeks()\n",
    "\n",
    "for ind, event_dict in enumerate(event_list_dict):\n",
    "    event_ms_time_stamp = event_ms_time_stamps[ind]\n",
    "    result = client.publish_event(project_id, model_id, event_dict, event_time_stamp=event_ms_time_stamp)\n",
    "    \n",
    "    clear_output(wait = True)\n",
    "    readable_timestamp = datetime.datetime.fromtimestamp(event_ms_time_stamp/1000.0)\n",
    "    \n",
    "    print(f'Sending {ind+1} / {NUM_EVENTS_TO_SEND} \\n{readable_timestamp} UTC: \\n{event_dict}')\n",
    "    time.sleep(0.01)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Note**: In the case that labels are ingested in a future point, an event can be updated by calling:\n",
    "\n",
    "- `res = fiddler_api.publish_event(project_id, model_id, event, event_id: customer, update_event=True, event_time_stamp=row['__occurred_at'])`\n",
    "\n",
    "By setting the `update_event` flag to be true, the event identifed by `event_id` will be updated with whatever additional information you pass in through `event`, including a target label. See [here](https://docs.fiddler.ai/api-reference/python-package/#publish-event) for more details.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Batch data example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to publish a batch of logs. Currently, we support batch publishing a **Pandas Dataframe**, or a **Parquet file hosted on an S3 bucket**. For this example, we will go with the second option.\n",
    "\n",
    "This Parqet file contains rows containing fields that are corresponding to:\n",
    "\n",
    "- inputs \n",
    "- predictions\n",
    "- labels (targets)\n",
    "- decisions\n",
    "\n",
    "We can find the fields that will be utilized by consulting our `ModelInfo` object (more info [here](#Streaming-data-example))\n",
    "\n",
    "For the purposes of this tutorial, we have a Parquet file uploaded to a public S3 bucket. While normally you would need to pass in an `auth_context` to access a private bucket, we do not require this step for the public S3 bucket. Commented code is left in to show how you would access a private S3 bucket.\n",
    "\n",
    "Note: the Parquet file also contains a **timestamp** column that we explicitly feed into the function to accurately reflect the time that our events occured. This **timestamp** can either be in the form:\n",
    "- `TIMESTAMP %Y-%m-%d %H:%M:%S.%f` (e.g. `2021-01-31 03:32:53.142000`)\n",
    "- `EPOCH_TIME` (e.g. `1613087108`)\n",
    "\n",
    "**The Parquet file we will be using has pre-configured timestamps that cover the entirety of January 2021.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# For this tutorial, we will be accessing a public S3 bucket. To read private S3 buckets, the following\n",
    "#  credentials dictionary will be needed\n",
    "auth_context = {'aws_access_key_id': '___',\n",
    "            'aws_secret_access_key': '___',\n",
    "            'aws_session_token': '___'\n",
    "            }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll only publish one file for this tutorial, but the list can be expanded to publish as many parquet\n",
    "# files as desired\n",
    "# This Parquet file has pre-configured timestamps that cover the entirety of January 2021.\n",
    "par_files = ['s3://fiddler-ai-public/datasets/quick_start_events.parquet']\n",
    "\n",
    "for par_file in par_files:\n",
    "    client.publish_events_batch(\n",
    "            project_id,\n",
    "            model_id,\n",
    "            par_file,\n",
    "            # auth_context=auth_context, # If using a private S3 bucket, uncomment this line and complete above cell\n",
    "            timestamp_field='timestamp'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing Monitoring Traffic\n",
    "We can now consult our Fiddler instance to visualize our monitoring results. We can see our newly created project within the Projects Overview section:\n",
    "\n",
    "<img src=\"images/qs_projects.png\" width=1000 height=1000 />\n",
    "\n",
    "Within our project, we can click `gradient_boosting_regressor` to see our model we created. From there, we can see the traffic that reflects the events we sent by going to the Monitor Section at the top:\n",
    "\n",
    "<img src=\"images/qs_monitoring.png\" width=1000 height=1000 />\n",
    "\n",
    "For a walkthrough to learn more about navigating the product, please consult our [Product Tour](https://docs.fiddler.ai/product-tour/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
